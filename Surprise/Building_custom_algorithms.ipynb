{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Creating your own prediction algorithm is pretty simple: an algorithm is nothing but a class derived from AlgoBase that has an estimate method. This is the method that is called by the predict() method. It takes in an inner user id, an inner item id (see this note), and returns the estimated rating r^ui:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import AlgoBase\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "class MyOwnAlgorithm(AlgoBase):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Always call base method before doing anything.\n",
    "        AlgoBase.__init__(self)\n",
    "        \n",
    "    def estimate(self,u,i):\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = MyOwnAlgorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm MyOwnAlgorithm on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.2447  1.2486  1.2380  1.2423  1.2470  1.2441  0.0038  \n",
      "MAE (testset)     1.0038  1.0040  0.9952  1.0011  1.0041  1.0017  0.0034  \n",
      "Fit time          0.00    0.02    0.02    0.02    0.02    0.01    0.01    \n",
      "Test time         0.08    0.06    0.05    0.06    0.06    0.06    0.01    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([1.24472889, 1.24863926, 1.23798223, 1.24227614, 1.24703649]),\n",
       " 'test_mae': array([1.00385, 1.004  , 0.9952 , 1.00115, 1.0041 ]),\n",
       " 'fit_time': (0.0,\n",
       "  0.015622615814208984,\n",
       "  0.015620708465576172,\n",
       "  0.015618562698364258,\n",
       "  0.015624046325683594),\n",
       " 'test_time': (0.07807540893554688,\n",
       "  0.062484025955200195,\n",
       "  0.04686689376831055,\n",
       "  0.062480926513671875,\n",
       "  0.062484025955200195)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(algo,data,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This algorithm is the dumbest we could have thought of: it just predicts a rating of 3, regardless of users and items.\n",
    "\n",
    "    If you want to store additional information about the prediction, you can also return a dictionary with given details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(self,u,i):\n",
    "    details = {'info1':'That was',\n",
    "               'info2':'easy stuff:)'}\n",
    "    return 3,details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This dictionary will be stored in the prediction as the details field and can be used for later analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The fit method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now, let’s make a slightly cleverer algorithm that predicts the average of all the ratings of the trainset. As this is a constant value that does not depend on current user or item, we would rather compute it once and for all. This can be done by defining the fit method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyOwnAlgorithm(AlgoBase):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Always call base method before doing anything.\n",
    "        AlgoBase.__init__(self)\n",
    "        \n",
    "    def fit(self,trainset):\n",
    "        \n",
    "        # Here again: call base method before doing anything.\n",
    "        AlgoBase.fit(self,trainset)\n",
    "        \n",
    "        # Compute the average rating. We might as well use the \n",
    "        # trainset.global_mean attribute ;)\n",
    "        self.the_mean = np.mean([r for (_, _, r) in self.trainset.all_ratings()])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def estimate(self,u,i):\n",
    "        \n",
    "        return self.the_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The fit method is called e.g. by the cross_validate function at each fold of a cross-validation process, (but you can also call it yourself). Before doing anything, you should call the base class fit() method.\n",
    "\n",
    "    Note that the fit() method returns self. This allows to use expression like algo.fit(trainset).test(testset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The trainset attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Once the base class fit() method has returned, all the info you need about the current training set (rating values, etc…) is stored in the self.trainset attribute. This is a Trainset object that has many attributes and methods of interest for prediction.\n",
    "\n",
    "    To illustrate its usage, let’s make an algorithm that predicts an average between the mean of all ratings, the mean rating of the user and the mean rating for the item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(self,u,i):\n",
    "    \n",
    "    sum_means = self.trainset.global_mean\n",
    "    div = 1\n",
    "    \n",
    "    if self.trainset.knows_user(u):\n",
    "        sum_means += np.mean([r for (_, r) in self.trainset.ur[u]])\n",
    "        div += 1\n",
    "    if self.trainset.knows_item(i):\n",
    "        sum_means += np.mean([r for (_, r) in self.trainset.ir[i]])\n",
    "        div += 1\n",
    "        \n",
    "    return sum_means / div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Note that it would have been a better idea to compute all the user means in the fit method, thus avoiding the same computations multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. When the prediction is impossible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    It’s up to your algorithm to decide if it can or cannot yield a prediction. If the prediction is impossible, then you can raise the PredictionImpossible exception. You’ll need to import it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import PredictionImpossible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This exception will be caught by the predict() method, and the estimation r^ui will be set according to the default_prediction() method, which can be overridden. By default, it returns the average of all ratings in the trainset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Using similarities and baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Should your algorithm use a similarity measure or baseline estimates, you’ll need to accept bsl_options and sim_options as parameters to the __init__ method, and pass them along to the Base class. See how to use these parameters in the Using prediction algorithms section.\n",
    "\n",
    "    Methods compute_baselines() and compute_similarities() can be called in the fit method (or anywhere else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From file examples/building_custom_algorithms/.with_baselines_or_sim.py\n",
    "class MyOwnAlgorithm(AlgoBase):\n",
    "    \n",
    "    def __init__(self,sim_options={},bsl_options={}):\n",
    "        \n",
    "        AlgoBase.__init__(self,sim_options=sim_options,bsl_options=bsl_options)\n",
    "        \n",
    "    def fit(self,trainset):\n",
    "        \n",
    "        AlgoBase.fit(self,trainset)\n",
    "        \n",
    "        # compute baselines and similarities\n",
    "        self.bu, self.bi = self.compute_baselines()\n",
    "        self.sim = self.compute_similarities()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def estimate(self,u,i):\n",
    "        \n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible('User and/or item is unknown.')\n",
    "            \n",
    "        # Compute similarities between u and v, where v describes all other\n",
    "        # users that have also rated item i.\n",
    "        neighbors = [(v, self.sim[u, v]) for (v, r) in self.trainset.ir[i]]\n",
    "        # Sort these neighbors by similarity\n",
    "        neighbors = sorted(neighbors, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        print('The 3 nearest neighbors of user', str(u), 'are:')\n",
    "        for v, sim_uv in neighbors[:3]:\n",
    "            print('user {0:} with sim {1:1.2f}'.format(v, sim_uv))\n",
    "\n",
    "        # ... Aaaaand return the baseline estimate anyway ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Feel free to explore the prediction_algorithms package source to get an idea of what can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
