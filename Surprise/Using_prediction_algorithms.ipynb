{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Surprise provides a bunch of built-in algorithms. All algorithms derive from the AlgoBase base class, where are implemented some key methods (e.g. predict, fit and test). The list and details of the available prediction algorithms can be found in the prediction_algorithms package documentation.\n",
    "\n",
    "    Every algorithm is part of the global Surprise namespace, so you only need to import their names from the Surprise package, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic\n",
    "algo = KNNBasic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Some of these algorithms may use baseline estimates, some may use a similarity measure. We will here review how to configure the way baselines and similarities are computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Baselines estimates configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This section only applies to algorithms (or similarity measures) that try to minimize the following regularized squared error (or equivalent):\n",
    "\n",
    "    ∑rui∈Rtrain(rui−(μ+bu+bi))2+λ(b2u+b2i).\n",
    "    For algorithms using baselines in another objective function (e.g. the SVD algorithm), the baseline configuration is done differently and is specific to each algorithm. Please refer to their own documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    First of all, if you do not want to configure the way baselines are computed, you don’t have to: the default parameters will do just fine. If you do want to well… This is for you.\n",
    "\n",
    "    You may want to read section 2.1 of [Kor10] to get a good idea of what are baseline estimates.\n",
    "\n",
    "    Baselines can be estimated in two different ways:\n",
    "\n",
    "        Using Stochastic Gradient Descent (SGD).\n",
    "        Using Alternating Least Squares (ALS).\n",
    "        \n",
    "    You can configure the way baselines are computed using the bsl_options parameter passed at the creation of an algorithm. This parameter is a dictionary for which the key 'method' indicates the method to use. Accepted values are 'als' (default) and 'sgd'. Depending on its value, other options may be set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    For ALS:\n",
    "\n",
    "        'reg_i': The regularization parameter for items. Corresponding to λ2 in [Kor10]. Default is 10.\n",
    "        'reg_u': The regularization parameter for users. Corresponding to λ3 in [Kor10]. Default is 15.\n",
    "        'n_epochs': The number of iteration of the ALS procedure. Default is 10. Note that in [Kor10], what is described is a single iteration ALS process.\n",
    "        \n",
    "    And for SGD:\n",
    "\n",
    "        'reg': The regularization parameter of the cost function that is optimized, corresponding to λ1 and then λ5 in [Kor10] Default is 0.02.\n",
    "        'learning_rate': The learning rate of SGD, corresponding to γ in [Kor10]. Default is 0.005.\n",
    "        'n_epochs': The number of iteration of the SGD procedure. Default is 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    For both procedures (ALS and SGD), user and item biases (bu and bi) are initialized to zero.\n",
    "\n",
    "Usage examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ALS\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BaselineOnly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f23c0ef84715>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                \u001b[1;34m'reg_i'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                }\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0malgo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaselineOnly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbsl_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbsl_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'BaselineOnly' is not defined"
     ]
    }
   ],
   "source": [
    "# From file examples/baselines_conf.py\n",
    "print('Using ALS')\n",
    "bsl_options = {'method': 'als',\n",
    "               'n_epochs': 5,\n",
    "               'reg_u': 12,\n",
    "               'reg_i': 5\n",
    "               }\n",
    "algo = BaselineOnly(bsl_options=bsl_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SGD\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BaselineOnly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7f351ca6c0fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                \u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m.00005\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                }\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0malgo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaselineOnly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbsl_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbsl_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'BaselineOnly' is not defined"
     ]
    }
   ],
   "source": [
    "# From file examples/baselines_conf.py\n",
    "print('Using SGD')\n",
    "bsl_options = {'method': 'sgd',\n",
    "               'learning_rate': .00005,\n",
    "               }\n",
    "algo = BaselineOnly(bsl_options=bsl_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Note that some similarity measures may use baselines, such as the pearson_baseline similarity. Configuration works just the same, whether the baselines are used in the actual prediction r^ui or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From file examples/baselines_conf.py\n",
    "bsl_options = {'method': 'als',\n",
    "               'n_epochs': 20,\n",
    "               }\n",
    "sim_options = {'name': 'pearson_baseline'}\n",
    "algo = KNNBasic(bsl_options=bsl_options, sim_options=sim_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads us to similarity measure configuration, which we will review right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Similarity measure configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Many algorithms use a similarity measure to estimate a rating. The way they can be configured is done in a similar fashion as for baseline ratings: you just need to pass a sim_options argument at the creation of an algorithm. This argument is a dictionary with the following (all optional) keys:\n",
    "\n",
    "    'name': The name of the similarity to use, as defined in the similarities module. Default is 'MSD'.\n",
    "    'user_based': Whether similarities will be computed between users or between items. This has a huge impact on the performance of a prediction algorithm. Default is True.\n",
    "    'min_support': The minimum number of common items (when 'user_based' is 'True') or minimum number of common users (when 'user_based' is 'False') for the similarity not to be zero. Simply put, if |Iuv|<min_support then sim(u,v)=0. The same goes for items.\n",
    "    'shrinkage': Shrinkage parameter to apply (only relevant for pearson_baseline similarity). Default is 100.\n",
    "    \n",
    "    Usage examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'cosine',\n",
    "              'user_based':False # compute similarities between items\n",
    "              }\n",
    "algo = KNNBasic(sim_options=sim_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'pearson_baseline',\n",
    "              'shrinkage': 0 # no shrinkage\n",
    "              }\n",
    "algo = KNNBasic(sim_options=sim_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
